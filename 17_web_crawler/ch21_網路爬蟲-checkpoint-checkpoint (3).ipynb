{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - ***[[數據分析] Python爬取競爭對手Google搜尋關鍵字佈局和文案](https://www.maxlist.xyz/2018/09/13/crawl_google_search/)\n",
    "\n",
    "#### 爬自己和競爭對手的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [Day14：Google Trends API (unofficial) 介紹及實作｜Kearch 1.0 爬蟲關鍵字報表工具](https://ithelp.ithome.com.tw/articles/10195071)\n",
    "#### - 生成JSON格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import json\n",
    "\n",
    "pytrend = TrendReq(hl='en-US', tz=360)\n",
    "pytrend.build_payload(\n",
    "    kw_list=['Donald Trump', 'Obama'], \n",
    "    cat=0, \n",
    "    timeframe='today 12-m', \n",
    "    geo='US', \n",
    "    gprop='')\n",
    "\n",
    "pytrend.interest_over_time()\n",
    "#pytrend.interest_over_time().get('Obama') #只想得到其中一個\n",
    "\n",
    "\n",
    "# 將剛剛的數據(python list)化成json物件方便後續處理：\n",
    "preload = json.loads(pytrend.interest_over_time().to_json(orient='table'))['data']\n",
    "print(json.dumps(preload, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [在 Python 中使用 pytrends 獲取 Google 搜尋趨勢的結果](https://clay-atlas.com/blog/2020/02/11/python-chinese-tutorial-package-pytrends-goolge-trends/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2019-12-25     0\n",
       "2019-12-26     0\n",
       "2019-12-27     0\n",
       "2019-12-28     0\n",
       "2019-12-29     0\n",
       "              ..\n",
       "2020-03-18    61\n",
       "2020-03-19    43\n",
       "2020-03-20    44\n",
       "2020-03-21    25\n",
       "2020-03-22    33\n",
       "Name: 陳時中, Length: 89, dtype: int32"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytrends.request import TrendReq\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "pytrend = TrendReq(hl='en-US', tz=360)\n",
    "keywords = ['陳時中', '陳建仁', '唐鳳']\n",
    "pytrend.build_payload(\n",
    "     kw_list=keywords,\n",
    "     cat=0,\n",
    "     timeframe='today 3-m',\n",
    "     geo='TW',\n",
    "     gprop='news')\n",
    "\n",
    "pytrend.interest_over_time_df = pytrend.interest_over_time()\n",
    "pytrend.interest_over_time_df['陳時中']\n",
    "#pprint(pytrend.interest_over_time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [[爬蟲筆記]Python Selenium爬蟲教學：實作商品庫存爬取](https://www.maxlist.xyz/2018/05/04/python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** - [[數據分析] 如何利用 Google Trend (搜尋趨勢) 挑選爆款商品](https://www.maxlist.xyz/2018/06/17/python_google_trend/)\n",
    "<br/>\n",
    "- 第一步：Python爬取資料<br/>\n",
    "- 第二步：資料清理<br/>\n",
    "- 第三步：資料篩選<br/>\n",
    "- 第四步：建立預測模型<br/>\n",
    "- 結論：Google Trend 與 Google Analytics 數據交叉比較<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [9.6. random — Generate pseudo-random numbers¶](https://docs.python.org/3.1/library/random.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['單車', '單車 環島', '迪卡儂', 'isPartial']\n"
     ]
    }
   ],
   "source": [
    "# - 第二步：資料清理\n",
    "\n",
    "## get商品熱度資料\n",
    "\n",
    "interest_over_time_df_str = [str(p) for p in interest_over_time_df ]\n",
    "#print(interest_over_time_df_str)\n",
    "#print(type(interest_over_time_df_str))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## get年月日資料\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "男性自行車\n",
      "環島\n",
      "迪卡儂\n",
      "女性自行車\n",
      "BTW\n",
      "第六個\n",
      "第七個\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Code Test : for\n",
    "\n",
    "# for 迴圈 01\n",
    "data = [r\"男性自行車\",r\"環島\",r\"迪卡儂\",r\"女性自行車\",r\"BTW\",r\"第六個\",r\"第七個\"]\n",
    "for keywords in data:\n",
    "    print(keywords)\n",
    "    \n",
    "\n",
    "data[2]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Test : PANDAS 数据合并与重塑（join/merge篇）\n",
    "https://blog.csdn.net/weixin_37226516/article/details/64137043\n",
    "    \n",
    "\n",
    "# 類似sql中的join     \n",
    "for j in range(len(total_list)):\n",
    "    df_list = pd.merge(total_list[1], total_list[], how='left', on=['date'])\n",
    "    print(df_list)\n",
    "\n",
    "    \n",
    "# 寬轉長\n",
    "\n",
    "mydata1=pList_df.melt(\n",
    "id_vars=\"\",   #要保留的主字段\n",
    "var_name=\"\",                     #拉长的分类变量\n",
    "value_name=[data]                 #拉长的度量值名称\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二步:資料清理 \n",
    "# pandas中索引的使用 https://blog.csdn.net/qq1483661204/article/details/77587881\n",
    "#len(df4.iloc[ : ])\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "trends_num = []\n",
    "df4_df = [str(p) for p in df4_df]\n",
    "print(df4_df)\n",
    "\n",
    "#for p in df4_df:\n",
    "#    if len(re.findall('[0-9]+',p)) > 0:\n",
    "#        trends_num.append(p)\n",
    "\n",
    "#trends_num_arr = np.array(trends_num, dtype=int)\n",
    "#arr_split = np.split(trends_num_arr, totle_item)\n",
    "#trends_dict = {}\n",
    "#for i in range(totle_item):\n",
    "#    trends_dict[data[keywords]][i] = arr_split[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "data = [('Peter', 18, 7), \n",
    "        ('Riff', 15, 6), \n",
    "        ('John', 17, 8), \n",
    "        ('Michel', 18, 7), \n",
    "         ('Sheli', 17, 5) ] \n",
    "\n",
    "df = pd.DataFrame(data, columns =['Team', 'Age', 'Score']) \n",
    "#print(df)\n",
    "a = df.pivot('Team','Age', 'Score') \n",
    "#print(a) \n",
    "\n",
    "#dfk = df.pivot('Team','Score','Age')\n",
    "#print(dfk)\n",
    "#print(dfk)\n",
    "ska = a.stack()\n",
    "print(ska)\n",
    "uska = ska.unstack()\n",
    "print(uska)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Pandas_Cheat_Sheet.pdf](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1    1   2    3    4   5    6    7     8\n",
      "0    date  None  陳時中  環島  獨輪車  登山車  火車  梨泰院   新冠  新垣結衣\n",
      "1    2015    03    0  67   53   65  98    0    0    12\n",
      "2    2015    04    0  56   44   72  73    0    0     7\n",
      "3    2015    04    0  56   43   59  68    0    0     7\n",
      "4    2015    04    0  44   56   80  64    0    0     6\n",
      "..    ...   ...  ...  ..  ...  ...  ..  ...  ...   ...\n",
      "257  2020    02   51  50   20   38  48   53   62     4\n",
      "258  2020    03   41  34   20   51  40   57   63     7\n",
      "259  2020    03   41  36   30   48  38   72   55     6\n",
      "260  2020    03   71  36   20   57  37   82   98    11\n",
      "261  2020    03   55  37   68   80  35  100  100     9\n",
      "\n",
      "[262 rows x 10 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-b1352c0e77db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m#result.tail()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;31m#dt_1 = pd.DataFrame(dt_1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;31m#result.to_csv(path_or_buf='C:/Users/MichaelCHEN/Desktop/csvfile3')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5268\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5269\u001b[0m         ):\n\u001b[1;32m-> 5270\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5271\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2041\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"string\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2098\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can only use .str accessor with string values!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2099\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "# 先透過匯出csv再匯入的方式來處理\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dt_1 = []\n",
    "dt_2 = []\n",
    "dt_3 = []\n",
    "dt_4 = []\n",
    "dt_5 = []\n",
    "dt_6 = []\n",
    "empty = []\n",
    "\n",
    "with open('csvfile2.csv', encoding='utf-8') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "    # 迴圈輸出 每一列\n",
    "    for row in rows:\n",
    "        empty.append(row)\n",
    "        #empty.extend(row)\n",
    "        #print(empty)\n",
    "               \n",
    "            \n",
    "empty_df = pd.DataFrame(empty)        \n",
    "#print(empty_df)\n",
    "  #empty_df.head()\n",
    "  #print(len(empty_df))\n",
    "date_value = empty_df[0][0:262]\n",
    "  #date_value_df = pd.DataFrame(date_value)   \n",
    "  #print(date_value)\n",
    "  #print((date_value[1][5:7]))\n",
    "  #print(type(date_value[1]))\n",
    "  #print(len(empty)) 262\n",
    "  #print(len(empty_df)) 262\n",
    "\n",
    "\n",
    "#先處理日期部分,將年與月拆分出來\n",
    "\n",
    "for i in range(0,len(empty_df)): #取1已經拿掉表頭\n",
    "    dt_1 = date_value[i][0:4].split()\n",
    "    dt_2 = date_value[i][5:7].split()\n",
    "    dt_3.append(dt_1+dt_2)\n",
    "\n",
    "#print(type(dt_3))\n",
    "dt_3_df = pd.DataFrame(dt_3)\n",
    "#print(type(dt_3_df))\n",
    "#print(dt_3_df)\n",
    "\n",
    "    \n",
    "#再將其他部分合併進來    \n",
    "#寫個迴圈改善\n",
    "\n",
    "df_kk = empty_df[1]\n",
    "df_kk_df = pd.DataFrame(df_kk)\n",
    "\n",
    "df_kk_2 = empty_df[2]\n",
    "df_kk_df_2 = pd.DataFrame(df_kk_2)\n",
    "\n",
    "df_kk_3 = empty_df[3]\n",
    "df_kk_df_3 = pd.DataFrame(df_kk_3)\n",
    "\n",
    "df_kk_4 = empty_df[4]\n",
    "df_kk_df_4 = pd.DataFrame(df_kk_4)\n",
    "\n",
    "df_kk_5 = empty_df[5]\n",
    "df_kk_df_5 = pd.DataFrame(df_kk_5)\n",
    "\n",
    "df_kk_6 = empty_df[6]\n",
    "df_kk_df_6 = pd.DataFrame(df_kk_6)\n",
    "\n",
    "df_kk_7 = empty_df[7]\n",
    "df_kk_df_7 = pd.DataFrame(df_kk_7)\n",
    "\n",
    "df_kk_8 = empty_df[8]\n",
    "df_kk_df_8 = pd.DataFrame(df_kk_8)\n",
    "\n",
    "\n",
    "frames = [dt_3_df, df_kk_df,df_kk_df_2,df_kk_df_3,df_kk_df_4,df_kk_df_5,df_kk_df_6,df_kk_df_7,df_kk_df_8]\n",
    "result = pd.concat(frames, axis=1)\n",
    "print(result)\n",
    "#print(type(result))\n",
    "#result.head()\n",
    "#result.tail()\n",
    "\n",
    "result[np.negative(pd.Series(result.columns).str.contains('index'))]\n",
    "#dt_1 = pd.DataFrame(dt_1)\n",
    "#result.to_csv(path_or_buf='C:/Users/MichaelCHEN/Desktop/csvfile3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-42-60eaa74cdffb>, line 56)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-60eaa74cdffb>\"\u001b[1;36m, line \u001b[1;32m56\u001b[0m\n\u001b[1;33m    #df4.to_csv(path_or_buf='C:/Users/MichaelCHEN/Desktop/csvfile2')\u001b[0m\n\u001b[1;37m                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# 第一步：Python爬取資料 - 超過 google 查詢上限的方法\n",
    "\n",
    "from pytrends.request import TrendReq\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "#  ,'獨輪車',登山車','火車','梨泰院','新冠','新垣結衣','男性自行車'\n",
    "data = ['陳時中','環島']\n",
    "#有些詞彙沒有搜尋熱度 - 暫不處理 '男性自行車','武漢肺炎','女性自行車'\n",
    "#print(len(data)-1)\n",
    "pList = []\n",
    "\n",
    "\n",
    "for keywords in data:\n",
    "    #print(keywords)\n",
    "    keywords_list = [keywords]\n",
    "    pytrend = TrendReq(hl='en-US', tz=360)\n",
    "    pytrend.build_payload(keywords_list, cat=0, timeframe='today 5-y', geo='TW', gprop='')\n",
    "    interest_over_time_df = pytrend.interest_over_time()[keywords]\n",
    "    pList.append(interest_over_time_df)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#print(type(pList))\n",
    "pList_df = pd.DataFrame(pList)\n",
    "\n",
    "# 欄與列的整理\n",
    "df3 = pList_df.stack(0)\n",
    "df4 = df3.unstack(0)\n",
    "#print(len(df4))\n",
    "\n",
    "#df4.head()\n",
    "\n",
    "#print(df4)\n",
    "df4.index.set_names(['L1', 'L2'], inplace=True\n",
    "\n",
    "# df4[\"陳時中\"] = df4[\"陳時中\"].replace(0,99) # 不是字串,不用.str\n",
    "\n",
    "#print(len(df4.iloc[ : ]))\n",
    "\n",
    "# 匯成JSON檔案\n",
    "#preload = json.loads(df4.to_json(orient='table'))\n",
    "#print(json.dumps(preload, ensure_ascii=False))\n",
    "\n",
    "\n",
    "## 輸出\n",
    "#print(len(pList))\n",
    "#print(pList_df)\n",
    "#df4.to_csv(path_or_buf='C:/Users/MichaelCHEN/Desktop/csvfile2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步：Python爬取資料 - 不超過 google 查詢上限可行\n",
    "\n",
    "from pytrends.request import TrendReq\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "# 超過 5 個 Google 不給回應,需要寫迴圈\n",
    "# 5年方面則是沒有問題\n",
    "\n",
    "\n",
    "keywords_list = ['男性自行車','環島','迪卡儂','女性自行車','獨輪車','摩托車','登山車']\n",
    "#print(len(keywords_list))\n",
    "#print(keywords_list[2])\n",
    "total_list = []\n",
    "df_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(keywords_list)-1): \n",
    "    #print(len(keywords_list)-1)\n",
    "    y = keywords_list[i]\n",
    "    x.append(y)\n",
    "    #print(y)\n",
    "    pytrend = TrendReq(hl='en-US', tz=360)\n",
    "    pytrend.build_payload(kw_list=x, cat=0, timeframe='today 5-y', geo='TW', gprop='') #自帶取出字串功能,所以用[i]會變成個別的字\n",
    "    interest_over_time_df = pytrend.interest_over_time()\n",
    "    #print(interest_over_time_df)   \n",
    "    #print 出來雖然keywords都有出現,但輸出成csv的時候就只有一個,需要 .append()\n",
    "    #print(type(interest_over_time_df))\n",
    "    total_list.append(interest_over_time_df)     #e改用extend p.6-28\n",
    "    \n",
    "    \n",
    "#print(len(total_list))\n",
    "#print(type(total_list))\n",
    "print(total_list)\n",
    "#print(total_list[2])\n",
    "    \n",
    "\n",
    "#for j in range(len(total_list)):\n",
    "#    df_list.append(total_list[j])\n",
    "\n",
    "\n",
    "#print(df_list)\n",
    "\n",
    "    \n",
    "    \n",
    "# 輸出 csv 格式\n",
    "#total_list_df = pd.DataFrame(total_list)\n",
    "#print(type(total_list_df))\n",
    "#total_list_df.to_csv(path_or_buf='C:/Users/MichaelCHEN/Desktop/csvfile')\n",
    "    # 輸出 JSON\n",
    "    #preload = json.loads(pytrend.interest_over_time().to_json(orient='table'))['data']\n",
    "    #print(json.dumps(preload, ensure_ascii=False))    \n",
    "    \n",
    "    # 匯成JSON檔案\n",
    "    #preload = json.loads(pytrend.interest_over_time().to_json(orient='table'))['data']\n",
    "    #print(json.dumps(preload, ensure_ascii=False))\n",
    "    \n",
    "    # 匯成CSV檔至本機\n",
    "    #interest_over_time_df.to_csv(path_or_buf='C:/Users/MichaelCHEN/Desktop/csvfile')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### [[python爬蟲] pytube 抓Youtube影片](http://jm191981111.blogspot.com/2019/07/python-pytube-youtube.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下載Youtube影片\n",
      " 影片標題：雪肌精「冷やし雪肌精」篇(メイキング）\n",
      "[<Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:/Users/MichaelCHEN/Desktop/youtube_download\\\\雪肌精「冷やし雪肌精」篇(メイキング）.mp4'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "yt=YouTube(\"https://www.youtube.com/watch?v=gX3r51KQUoc\")\n",
    "print(\"下載Youtube影片\\n 影片標題：\"+yt.title)\n",
    "\n",
    "GetVideo=yt.streams.filter(file_extension=\"mp4\",resolution=\"720p\").all()\n",
    "print(GetVideo)\n",
    "stream = GetVideo[0]\n",
    "stream.download(\"C:/Users/MichaelCHEN/Desktop/youtube_download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *[Python 類別的定義與使用 - Class Attributes By 彭彭](https://www.youtube.com/watch?v=uPKgQ3FoVtY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    x=3\n",
    "    def say():\n",
    "        print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.x+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "Test.say()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [使用selenium 抓取 Youtube 熱門影片 (PYTHON)](https://www.youtube.com/watch?v=y7T3_EHa_JY)\n",
    "- #### [twtrubiks/youtube-trends-spider](https://www.youtube.com/watch?v=y7T3_EHa_JY)\n",
    "- [Youtube Trends](https://youtube.com/trends/)\n",
    "- #### [Youtube Data Api](https://developers.google.com/youtube/v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [[Scrapy 爬蟲] 什麼是Scrapy以及為什麼要用Scrapy 爬取網頁?](https://www.youtube.com/watch?v=0pWJHy_fNWA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ [爬蟲實戰] 如何爬取PTT的網頁?](https://www.youtube.com/channel/UCFdTiwvDjyc62DBWrlYDtlQ/search?query=ptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Two - beautiful soup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://www.ptt.cc/bbs/Food/index.html')\n",
    "soup = BeautifulSoup(res.text)\n",
    "for entry in soup.select('.r-ent'):\n",
    "    #print(entry)\n",
    "    #print(entry.select('.title'))\n",
    "    #print(entry.select('.date'))\n",
    "    #print(entry.select('.date')[0].text)\n",
    "    print(entry.select('.date')[0].text, entry.select('.author')[0].text,entry.select('.title')[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step One - requests - get\n",
    "\n",
    "import requests\n",
    "\n",
    "res = requests.get('https://www.ptt.cc/bbs/Food/index.html')\n",
    "#以前會需要 verify = false\n",
    "\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ [Selenium] 如何使用 Selenium 自動下載漫畫 (1)?](https://www.youtube.com/watch?v=grZi9j4HKvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driverPath = 'C:/Users/MichaelCHEN/AppData/Local/Programs/Python/Python37/Scripts/chromedriver.exe'\n",
    "browser = webdriver.Chrome(driverPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [[爬蟲實戰] 如何告訴PTT我已滿18並順利抓取八卦版的文章 ?](https://www.youtube.com/watch?v=G5MDpnGsE-k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Three : Import Beautiful Soup\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Two : 解決 18 禁問題:在 get 前要先處理 post\n",
    "# 進入 F12 > Network > 按下over18?...頁面 > Headers > 按下 Yes -  ptt.cc/ask/over18 > From Data: from: bbs/Gossiping/index.html;yes:yes\n",
    "\n",
    "import requests\n",
    "\n",
    "payload = {\n",
    "    'from':'/bbs/Gossiping/index.html',\n",
    "    'yes':'yse'\n",
    "}\n",
    "rs = requests.session()\n",
    "res = rs.post('https://ptt.cc/ask/over18', data = payload)\n",
    "res = rs.get('https://www.ptt.cc/bbs/Gossiping/index.html') \n",
    "print(res.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step One : 進入前有18禁限制\n",
    "\n",
    "import requests\n",
    "res = requests.get('https://www.ptt.cc/bbs/Gossiping/index.html') # varify - 驗證\n",
    "print(res.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch.22_Selenium_網路爬蟲王者\n",
    "- Selenium 透過模仿整個瀏覽器去抓取資料\n",
    "- #### [Python-- Selenium用法](https://www.itread01.com/content/1541166610.html#8%EF%BC%9A%E8%8E%B7%E5%8F%96%E5%85%83%E7%B4%A0%E4%BF%A1%E6%81%AF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driverPath = 'C:/Users/MichaelCHEN/AppData/Local/Programs/Python/Python37/Scripts/chromedriver.exe'\n",
    "browser = webdriver.Chrome(driverPath)\n",
    "url = 'http://aaa.24ht.com.tw'\n",
    "browser.get(url)   # 網頁下載至瀏覽器\n",
    "\n",
    "\n",
    "#tag1 = browser.find_element_by_tag_name('title')\n",
    "#print(\"標籤名稱 = %s, 內容是 = %s\" % (tag1.tag_name,tag1.text))\n",
    "\n",
    "\n",
    "#tag2 = browser.find_element_by_id('author')\n",
    "#print(\"\\n標籤名稱 = %s, 內容是 = %s\" % (tag2.tag_name,tag2.text))\n",
    "\n",
    "tag3 = browser.find_element_by_id('content')\n",
    "for i in range(len(tag3)):\n",
    "    print(\"標籤名稱 = %s, 內容是 = %s\" % (tag3[i].tag_name,tag3[i].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch22_6.py # 找不到符合條件的元素時,執行例外處理 # 解決 ch22_5.py 報錯問題\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "driverPath = 'C:/Users/MichaelCHEN/AppData/Local/Programs/Python/Python37/Scripts/chromedriver.exe'\n",
    "browser = webdriver.Chrome(driverPath)\n",
    "url = 'http://aaa.24ht.com.tw'\n",
    "browser.get(url)   # 網頁下載至瀏覽器\n",
    "\n",
    "try:\n",
    "    tag = browser.find_element_by_id('main')\n",
    "    print(tag.tag_name)\n",
    "except:\n",
    "    print(\"沒有找到相符的元素\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch22_5.py # 找到元素\n",
    "from selenium import webdriver\n",
    "\n",
    "driverPath = 'C:/Users/MichaelCHEN/AppData/Local/Programs/Python/Python37/Scripts/chromedriver.exe'\n",
    "browser = webdriver.Chrome(driverPath)\n",
    "url = 'http://aaa.24ht.com.tw'\n",
    "browser.get(url)   # 網頁下載至瀏覽器\n",
    "tag = browser.find_element_by_id('content')\n",
    "print(tag.tag_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch22_5.py # 列出找不到元素,造成程式結束的實例\n",
    "from selenium import webdriver\n",
    "\n",
    "driverPath = 'C:/Users/MichaelCHEN/AppData/Local/Programs/Python/Python37/Scripts/chromedriver.exe'\n",
    "browser = webdriver.Chrome(driverPath)\n",
    "url = 'http://aaa.24ht.com.tw'\n",
    "browser.get(url)   # 網頁下載至瀏覽器\n",
    "tag = browser.find_element_by_id('main')\n",
    "print(tag.tag_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch22_4.py\n",
    "from selenium import webdriver\n",
    "\n",
    "driverPath = 'C:/Users/MichaelCHEN/AppData/Local/Programs/Python/Python37/Scripts/chromedriver.exe'\n",
    "browser = webdriver.Chrome(driverPath)\n",
    "url = 'http://www.google.com'\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch22_3.py\n",
    "from selenium import webdriver\n",
    "\n",
    "driverPath = 'C:/Users/MichaelCHEN/AppData/Local/Programs/Python/Python37/Scripts/chromedriver.exe'\n",
    "browser = webdriver.Chrome(driverPath)\n",
    "print(type(browser))\n",
    "\n",
    "\"\"\"\n",
    "Download and unzip chromedriver and put 'chromedriver.exe' in C:\\Python27\\Scripts and then you need not to provide the path of driver, just\n",
    "\n",
    "driver= webdriver.Chrome()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21-4 解析網頁使用 BeautifulSoup 模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列印BeautifulSoup物件資料型態 <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, bs4\n",
    "\n",
    "htmlFile = requests.get('http://www.yahoo.com.tw')\n",
    "#objSoup = bs4.BeautifulSoup(htmlFile.text,'html5lib')\n",
    "objSoup = bs4.BeautifulSoup(htmlFile.text,'lxml')\n",
    "# html.parser - 老舊方法,相容性較不好; lxml - 速度快,相容性佳; html5lib - 速度慢,解析力強\n",
    "print(\"列印BeautifulSoup物件資料型態\", type(objSoup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21-2 下載網頁資訊使用 requests 模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-7\n",
    "# iter_countent()\n",
    "import requests\n",
    "\n",
    "url='http://www.deepstone.com.tw'\n",
    "try:\n",
    "    htmlfile = requests.get(url)\n",
    "    print(\"下載成功\")\n",
    "except Exception as err:\n",
    "    print(\"網頁下載失敗: %s\" % err)\n",
    "#儲存網頁內容\n",
    "fn = 'out21_11.txt'\n",
    "with open(fn, 'wb') as file_Obj:    #以二進位儲存\n",
    "    for diskStorage in htmlfile.iter_content(10240): #Reponse物件處理\n",
    "        size = file_Obj.write(diskStorage)    #Reponse物件寫入\n",
    "        print(size)    #列出每次寫入大小\n",
    "    print(\"以 %s 儲存網頁HTML檔案成功\" % fn)\n",
    "\n",
    "\"\"\"\n",
    "由於網頁內容比較小,所以每次寫入檔案大小設為10240bytes,程式中with之後所開啟的是以二進位可寫入\"wb\"方式開啟,這是為了怕網頁內有Unicode碼.\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-6\n",
    "#爬蟲程式偽裝成瀏覽器\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64)\\\n",
    "            AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101\\\n",
    "            safari/537.36',}\n",
    "\n",
    "url = 'http://aaa.24ht.com.tw/'\n",
    "htmlfile = requests.get(url, headers=headers)\n",
    "htmlfile.raise_for_status()\n",
    "print(\"偽裝瀏覽器擷取網路資料成功\")\n",
    "           \n",
    "# \"\\\" 表下一行跟這一行是相同敘述\n",
    "           \n",
    "\"\"\"\n",
    "An EOL ( End of Line ) error indicates that the Python interpreter expected a \n",
    "particular character or set of characters to have occurred in a specific line \n",
    "of code, but that those characters were not found before the end of the line .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-5\n",
    "#網頁伺服器阻擋造成讀取錯誤\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'http://aaa.24ht.com.tw/'\n",
    "htmlfile = requests.get(url)\n",
    "htmlfile.raise_for_status()\n",
    "\n",
    "#HTTPError: 406 Client Error: Not Acceptable for url: http://aaa.24ht.com.tw/\n",
    "#406錯誤就是網頁伺服器阻擋,用raise_for_status()可以快速中斷協助我們偵測錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-4\n",
    "#下載網頁失敗的異常處理\n",
    "#Reponse 物件有 raise_for_status(),可以針對網址正確但是後續檔案名稱錯誤的狀況產生異常處理\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'http://www.yahoo.com/file_not_exit' # 不存在的內容\n",
    "htmlfile = requests.get(url)\n",
    "try:                              # try參考 p15-3 \n",
    "    htmlfile.raise_for_status()   #異常處理\n",
    "    print(\"下載成功\")\n",
    "except Exception as err:          # except參考 p15-11 # Exception - 常見的異常物件 p15-8 - 一般錯誤皆可使用\n",
    "    print(\"網頁下載失敗: %s\" % err)\n",
    "    \n",
    "    \n",
    "\"\"\"\"\n",
    "try:\n",
    "    指令           #預先設想可能引發錯誤異常的指令\n",
    "except 異常物件    \n",
    "    異常處理程序    #通常是指出異常原因,方便修正\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-3\n",
    "#搜尋網頁特定內容\n",
    "\n",
    "import requests\n",
    "import re\n",
    "url = 'https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0JYcG9MVlJYR2dKVVZ5Z0FQAQ/sections/CAQiXkNCQVNRQW9JTDIwdk1ESnFhblFTQlhwb0xWUlhHZ0pVVnlJT0NBUWFDZ29JTDIwdk1ESjJlRzRxR1FvWENoTk5UMVpKUlZOZlUwVkRWRWxQVGw5T1FVMUZJQUVvQUEqLggAKioICiIkQ0JBU0ZRb0lMMjB2TURKcWFuUVNCWHBvTFZSWEdnSlVWeWdBUAFQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant'\n",
    "htmlfile = requests.get(url)\n",
    "if htmlfile.status_code == requests.codes.ok:\n",
    "    pattern = input(\"請輸入欲搜尋的字串:\")  #pattern存放欲搜尋的字串  \n",
    "    #使用方法1\n",
    "    if pattern in htmlfile.text:\n",
    "         print(\"搜尋 %s 成功\" % pattern)  # %d , %s , % 的用法參考 p.4-4\n",
    "    else:\n",
    "         print(\"搜尋 %s 失敗\" % pattern)\n",
    "    #使用方法2\n",
    "    # re 模塊 http://blog.fantasy.codes/python/2013/07/02/py-re-module/\n",
    "    name = re.findall(pattern, htmlfile.text) #方法2\n",
    "    if name != None:\n",
    "        print(\"%s 出現 %d 次\" % (pattern, len(name)))\n",
    "    else:\n",
    "        print(\"%s 出現 0 次\" % pattern)\n",
    "\n",
    "else:\n",
    "    \n",
    "    print(\"網頁下載失敗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-2\n",
    "#列印網頁原始碼\n",
    "import requests\n",
    "url='http://decathlon.tw'\n",
    "htmlfile = requests.get(url)\n",
    "if htmlfile.status_code == requests.codes.ok:\n",
    "    print(\"取得網頁成功\")\n",
    "else:\n",
    "    print(\"取得網頁內容失敗\")\n",
    "print(htmlfile.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-2\n",
    "#擴充,取得網頁內容大小\n",
    "import requests\n",
    "url = 'https://www.decathlon.tw'\n",
    "htmlfile = requests.get(url)\n",
    "if htmlfile.status_code == requests.codes.ok:\n",
    "    print(\"取得網頁成功\")\n",
    "else:\n",
    "    print(\"取得網頁內容失敗\")\n",
    "print(\"網頁內容大小=\",len(htmlfile.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-2\n",
    "#Response\n",
    "import requests\n",
    "url='http://decathlon.tw'\n",
    "htmlfile = requests.get(url)\n",
    "if htmlfile.status_code == requests.codes.ok:\n",
    "    print(\"取得網頁成功\")\n",
    "else: \n",
    "    print(\"取得網頁內容失敗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-2-1\n",
    "#requests.get()\n",
    "#py -m pip install requests\n",
    "\n",
    "import requests\n",
    "\n",
    "url='http://decathlon.tw'\n",
    "htmlfile = requests.get(url)\n",
    "print(type(htmlfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-1-3\n",
    "#地址查詢地圖的程式設計\n",
    "import webbrowser\n",
    "address = input('請輸入地址:')\n",
    "webbrowser.open('http://www.google.com.tw/maps/place/' + address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-1-1\n",
    "#webbrowser\n",
    "import webbrowser\n",
    "webbrowser.open('www.google.com')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit14a35242e66642798e1bd2b036fbac02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
